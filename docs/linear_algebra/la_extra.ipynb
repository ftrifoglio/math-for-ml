{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra - Extra curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Componenent Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply all the learnings in this notebook to a simple example of PCA.\n",
    "\n",
    "We'll see applications of:\n",
    "- singular and non-singular matrices\n",
    "- determinants\n",
    "- dot products\n",
    "- linear transformations\n",
    "- eigenvectors and eigenvalues\n",
    "- vector projections\n",
    "\n",
    "This was inspired by Luis Serrano's [YT video](https://www.youtube.com/watch?v=g-Hb26agBFg) on PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "x1 = np.array([[1], [2], [3], [3], [5], [6]])\n",
    "x2 = (\n",
    "    10\n",
    "    + np.array([[1], [2], [3], [3], [5], [6]]) * 15\n",
    "    + rng.normal(0.0, 30.0, 6).reshape(-1, 1)\n",
    ")\n",
    "X = np.hstack([x1, x2])\n",
    "\n",
    "for r in range(X.shape[0]):\n",
    "    plt.quiver(\n",
    "        [0],\n",
    "        [0],\n",
    "        [X[r][0]],\n",
    "        [X[r][1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "plt.xlim(0, X[:, 0].max() * 1.05)\n",
    "plt.ylim(0, X[:, 1].max() * 1.05)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Samples as vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "for r in range(Z.shape[0]):\n",
    "    plt.quiver(\n",
    "        [0],\n",
    "        [0],\n",
    "        [Z[r][0]],\n",
    "        [Z[r][1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "plt.xlim(Z[:, 0].min() * 1.2, Z[:, 0].max() * 1.2)\n",
    "plt.ylim(Z[:, 1].min() * 1.2, Z[:, 1].max() * 1.2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.title(\"Effect of standardization on the vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the covariance matrix, whose eigenvectors represent the directions of the maximum variance (or better maximum correlation since we're using standardized data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(Z[:, 0], Z[:, 1], bias=True)\n",
    "cov_evecs = np.linalg.eig(cov)[1]\n",
    "cov_v1 = cov_evecs[:, 0]\n",
    "cov_v2 = cov_evecs[:, 1]\n",
    "print(f\"Determinant of transformation: {np.linalg.det(cov):.2f}\")\n",
    "print(f\"Dot product of eigenvectors: {np.dot(cov_v1, cov_v2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the covariance matrix represent a non-singular linear transformation.\n",
    "\n",
    "We can also see that the eigenvectors are orthogonal because their dot product is 0.\n",
    "\n",
    "> üîë The eigenvectors of a symmetric matrix are orthogonal\n",
    "\n",
    "Another property of symmetric matrices is that their transformation always entails some sort of stretching, which means that the eigenvalues are always real numbers (not complex numbers like in the case of rotations).\n",
    "\n",
    "Let's visualize the effect of the linear transformation induced by the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation(T, title, ax, basis=None, lim=5):\n",
    "    if basis is None:\n",
    "        e1 = np.array([[1], [0]])\n",
    "        e2 = np.array([[0], [1]])\n",
    "    else:\n",
    "        e1, e2 = basis\n",
    "    zero = np.zeros(1, dtype=\"int\")\n",
    "    c = \"tab:blue\"\n",
    "    c_t = \"tab:orange\"\n",
    "    ax.set_xticks(np.arange(-lim, lim))\n",
    "    ax.set_yticks(np.arange(-lim, lim))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "    _plot_vectors(e1, e2, c, ax)\n",
    "    ax.plot(\n",
    "        [zero, e2[0], e1[0] + e2[0], e1[0]],\n",
    "        [zero, e2[1], e1[1] + e2[1], e1[1]],\n",
    "        color=c,\n",
    "    )\n",
    "    _make_labels(e1, \"$e_1$\", c, y_offset=(-0.2, 1.0), ax=ax)\n",
    "    _make_labels(e2, \"$e_2$\", c, y_offset=(-0.2, 1.0), ax=ax)\n",
    "    e1_t = T(e1)\n",
    "    e2_t = T(e2)\n",
    "    _plot_vectors(e1_t, e2_t, c_t, ax)\n",
    "    ax.plot(\n",
    "        [zero, e2_t[0], e1_t[0] + e2_t[0], e1_t[0]],\n",
    "        [zero, e2_t[1], e1_t[1] + e2_t[1], e1_t[1]],\n",
    "        color=c_t,\n",
    "    )\n",
    "    _make_labels(e1_t, \"$T(e_1)$\", c_t, y_offset=(0.0, 1.0), ax=ax)\n",
    "    _make_labels(e2_t, \"$T(e_2)$\", c_t, y_offset=(0.0, 1.0), ax=ax)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def _make_labels(e, text, color, y_offset, ax):\n",
    "    e_sgn = 0.4 * np.array([[1] if i == 0 else i for i in np.sign(e)])\n",
    "    return ax.text(\n",
    "        e[0] - 0.2 + e_sgn[0],\n",
    "        e[1] + y_offset[0] + y_offset[1] * e_sgn[1],\n",
    "        text,\n",
    "        fontsize=12,\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "\n",
    "def _plot_vectors(e1, e2, color, ax):\n",
    "    ax.quiver(\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [e1[0], e2[0]],\n",
    "        [e1[1], e2[1]],\n",
    "        color=color,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def T(A, v):\n",
    "    w = A @ v\n",
    "    return w\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_transformation(\n",
    "    partial(T, cov),\n",
    "    title=\"Covariance matrix as a linear transformation\",\n",
    "    basis=(cov_v1.reshape(-1, 1), cov_v2.reshape(-1, 1)),\n",
    "    ax=ax,\n",
    "    lim=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better see the effect of the transformation, we've used a basis that its parallel to the eigenbasis of the transformation.\n",
    "\n",
    "$COV_{x_1x_2} = \\begin{bmatrix}1&&0.8\\\\0.8&&1\\end{bmatrix}$\n",
    "\n",
    "$\\vec{v_1} = \\begin{bmatrix}0.7\\\\0.7\\end{bmatrix}$\n",
    "\n",
    "$\\vec{v_2} = \\begin{bmatrix}-0.7\\\\0.7\\end{bmatrix}$\n",
    "\n",
    "$COV_{x_1x_2} \\cdot \\vec{v_1} = \\begin{bmatrix}1 \\times 0.7 + 0.8 \\times 0.7\\\\0.8 \\times 0.7 + 1 \\times 0.7\\end{bmatrix} = \\begin{bmatrix}1.3\\\\1.3\\end{bmatrix}$\n",
    "\n",
    "$COV_{x_1x_2} \\cdot \\vec{v_2} = \\begin{bmatrix}1 \\times -0.7 + 0.8 \\times 0.7\\\\0.8 \\times -0.7 + 1 \\times 0.7\\end{bmatrix} = \\begin{bmatrix}-0.14\\\\0.14\\end{bmatrix}$\n",
    "\n",
    "Let's overlay the eigenvectors on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(Z.shape[0]):\n",
    "    plt.quiver(\n",
    "        [0],\n",
    "        [0],\n",
    "        [Z[r][0]],\n",
    "        [Z[r][1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "plt.quiver(\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [cov_v1[0], cov_v2[0]],\n",
    "    [cov_v1[1], cov_v2[1]],\n",
    "    angles=\"xy\",\n",
    "    scale=1,\n",
    "    scale_units=\"xy\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(Z[:, 0].min() * 1.2, Z[:, 0].max() * 1.2)\n",
    "plt.ylim(Z[:, 1].min() * 1.2, Z[:, 1].max() * 1.2)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.title(\"Eigenvectors of the covariance matrix on the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the principal components (eigenvectors times eigenvalues). Of course, the first principal component is the one with the highest magnitude (eigenvalue).\n",
    "\n",
    "Given the nature of this data (positively correlated) we expect the first principal component to be the one corresponding to the eigenvector with signs [+, +] or [-, -]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvls, eigvcs = np.linalg.eig(cov)\n",
    "rank = np.argsort(-eigvls)\n",
    "eigvls = eigvls[rank]\n",
    "eigvcs = eigvcs[:, rank]\n",
    "pc = eigvcs * eigvls\n",
    "\n",
    "for r in range(Z.shape[0]):\n",
    "    plt.quiver(\n",
    "        [0],\n",
    "        [0],\n",
    "        [Z[r][0]],\n",
    "        [Z[r][1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "plt.quiver(\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [pc[:, 0][0], pc[:, 1][0]],\n",
    "    [pc[:, 0][1], pc[:, 1][1]],\n",
    "    angles=\"xy\",\n",
    "    scale=1,\n",
    "    scale_units=\"xy\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.annotate(\"PC1\", pc[:, 0], color=\"tab:orange\")\n",
    "plt.annotate(\"PC2\", pc[:, 1], color=\"tab:orange\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(Z[:, 0].min() * 1.2, Z[:, 0].max() * 1.2)\n",
    "plt.ylim(Z[:, 1].min() * 1.2, Z[:, 1].max() * 1.2)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.title(\"Eigenvectors stretched by their eigenvalues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project all the vectors onto the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 1\n",
    "proj_norm = np.dot(Z, eigvcs[:, :n_components])  # (6, 2) @ (2, 1) -> (6, 1)\n",
    "proj = np.dot(proj_norm, eigvcs[:, :n_components].T)  # (6, 1) @ (1, 2) -> (6, 2)\n",
    "plt.plot([Z[:, 0], proj[:, 0]], [Z[:, 1], proj[:, 1]], \"--\", color=\"tab:blue\")\n",
    "for r in range(Z.shape[0]):\n",
    "    plt.quiver(\n",
    "        [0],\n",
    "        [0],\n",
    "        [Z[r][0]],\n",
    "        [Z[r][1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "plt.quiver(\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [pc[:, 0][0], -pc[:, 0][0]],\n",
    "    [pc[:, 0][1], -pc[:, 0][0]],\n",
    "    angles=\"xy\",\n",
    "    scale=1,\n",
    "    scale_units=\"xy\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.scatter(proj[:, 0], proj[:, 1], color=\"tab:orange\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(Z[:, 0].min() * 1.2, Z[:, 0].max() * 1.2)\n",
    "plt.ylim(Z[:, 1].min() * 1.2, Z[:, 1].max() * 1.2)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.title(\"Projections to the eigenvector with the largest eigenvalue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting application to illustrate the importance of linear algebra is SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "group_1_centroid = [-2, -2]\n",
    "group_2_centroid = [2, 2]\n",
    "X = np.r_[\n",
    "    rng.standard_normal((20, 2)) + group_1_centroid,\n",
    "    rng.standard_normal((20, 2)) + group_2_centroid,\n",
    "]\n",
    "y = np.array([-1] * 20 + [1] * 20)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Data with 2 classes (perfect linear separation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find a line $wx+b$ such that\n",
    "\n",
    "$h(x_i) = \\begin{cases}1 &\\text{if }wx_i+b \\ge 0\\\\-1 &\\text{otherwise }\\end{cases} \\text{ for } i = 1, \\dots, m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(x1, b, w):\n",
    "    return (-w[0] * x1 - b) / w[1]\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"))\n",
    "x1 = np.linspace(-4, 4, 100)\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=0.2, w=np.array([0.5, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dotted\",\n",
    "    label=\"DB 1\",\n",
    ")\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=-0.1, w=np.array([-0.6, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"DB 2\",\n",
    ")\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=-0.5, w=np.array([0.2, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dashdot\",\n",
    "    label=\"DB 3\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Random Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we know DB 2 ($x_2 = -0.1 - 0.6x_1$) is the best one of the three, but why?\n",
    "\n",
    "We haven't seen this during the course, but the normalized distance between a point and a line is\n",
    "\n",
    "> üìê $d = \\cfrac{|w_1x_1 + w_2x_2 + b|}{\\sqrt{w_1^2 + w_2^2}} = \\cfrac{|w \\cdot x + b|}{\\|w\\|}$\n",
    "\n",
    "<img src=\"https://ds055uzetaobb.cloudfront.net/brioche/uploads/pHeSaRZIjr-untitled-drawing-7.png\" width=\"300px\">\n",
    "*Source: www.brilliant.org*\n",
    "\n",
    "$x_2 = -0.1 - 0.6x_1$ can be rewritten as $-0.6x_1 -1x_2 -0.1 = 0$ so we can get\n",
    "\n",
    "$d = \\cfrac{|-0.6x_1 -1x_2 -0.1|}{\\sqrt{-0.6^2 + -1^2}}$\n",
    "\n",
    "It turns out that the formula for $d$ (distance between a point and a line) has a vector projection proof.\n",
    "\n",
    "Let the point $P = (x^0_1, x^0_2)$ and let the given line have equation $w_1x_1 + w_2x_2 + b = 0$.  \n",
    "\n",
    "Also, let $Q = (x^1_1, x^1_2)$  be any point on this line and let the vector $\\vec{w} = \\langle w_1, w_2 \\rangle$ starting at point $Q$.\n",
    "\n",
    "For example, $Q$ could be $(x_1, -0.1 - 0.6x_1)$, and by arbitrarily fixing $x_1 = 0$ we get $Q = (0, -0.1)$.\n",
    "\n",
    "The vector $\\vec{w}$ is perpendicular to the line.\n",
    "\n",
    "The distance $d$ from the point $P$ to the line is equal to the length of the orthogonal projection of $\\overrightarrow{QP}$ onto $\\vec{w}$.\n",
    "\n",
    "$d = \\|\\overrightarrow{proj_{w}QP}\\| = \\cfrac{(\\vec{P} - \\vec{Q}) \\cdot \\vec{w}}{\\|\\vec{w}\\|} \\cfrac{\\vec{w}}{\\|\\vec{w}\\|}$\n",
    "\n",
    "The only difference to what we saw earlier during the course is that the vector $\\vec{P}$ starts at point $Q$ (a point on the line) and not the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-0.6, -1.0])\n",
    "P = X[0]\n",
    "Q = np.array([0, decision_boundary(0, b=-0.1, w=w)])\n",
    "proj_P = (np.dot(P - Q, w) / np.linalg.norm(w)) * (w / np.linalg.norm(w))\n",
    "\n",
    "x1 = np.linspace(-4, 4, 100)\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=-0.1, w=w),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"DB 2\",\n",
    ")\n",
    "arc = plt.Rectangle(\n",
    "    Q,\n",
    "    -0.2,\n",
    "    -0.2,\n",
    "    angle=np.degrees(np.arctan(w[0])),\n",
    "    fill=False,\n",
    "    edgecolor=\"k\",\n",
    ")\n",
    "plt.gca().add_patch(arc)\n",
    "plt.quiver(\n",
    "    [\n",
    "        Q[0],\n",
    "        Q[0],\n",
    "        P[0],\n",
    "        Q[0],\n",
    "    ],\n",
    "    [\n",
    "        Q[1],\n",
    "        Q[1],\n",
    "        P[1],\n",
    "        Q[1],\n",
    "    ],\n",
    "    [\n",
    "        P[0],\n",
    "        proj_P[0],\n",
    "        -proj_P[0],\n",
    "        w[0],\n",
    "    ],\n",
    "    [\n",
    "        P[1],\n",
    "        proj_P[1],\n",
    "        -proj_P[1],\n",
    "        w[1],\n",
    "    ],\n",
    "    angles=\"xy\",\n",
    "    scale=1,\n",
    "    scale_units=\"xy\",\n",
    "    fc=[\"tab:blue\", \"tab:green\", \"none\", \"tab:orange\"],\n",
    "    ec=[\"none\", \"none\", \"k\", \"none\"],\n",
    "    ls=[\"solid\", \"solid\", \"dashed\", \"solid\"],\n",
    "    linewidth=1,\n",
    ")\n",
    "plt.text(X[0, 0] - 0.3, X[0, 1] - 0.3, \"$P$\", color=\"tab:blue\", fontsize=12)\n",
    "plt.text(w[0] - 0.3, w[1], \"$w$\", color=\"tab:orange\", fontsize=12)\n",
    "plt.text(\n",
    "    proj_P[0] - 0.3, proj_P[1] - 0.3, \"$proj_{n}P$\", color=\"tab:green\", fontsize=12\n",
    ")\n",
    "plt.text(-3, 2.5, \"$w_1x_1 + w_2x_2 + b = 0$\", color=\"k\", fontsize=12)\n",
    "plt.text(-1.75, -0.5, \"$d$\", color=\"k\", fontsize=12)\n",
    "plt.text(Q[0] + 0.1, Q[1] + 0.1, \"$Q$\", color=\"k\", fontsize=12)\n",
    "plt.title(\"Projection proof of $d$\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.abs(P @ w.reshape(-1, 1) + -0.1) / np.linalg.norm(w)\n",
    "proof_d = np.linalg.norm(proj_P)\n",
    "assert np.isclose(d, proof_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest distance among all data points to the decision boundary is called margin.\n",
    "\n",
    "$\\gamma(w, b) = \\min\\cfrac{|wx_i+b|}{\\|w\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin(X, b, w):\n",
    "    return np.min(np.abs(X @ w.reshape(-1, 1) + b) / np.linalg.norm(w))\n",
    "\n",
    "\n",
    "print(f\"Margin of decision boundary 1: {margin(X, b=0.2, w=np.array([0.5, -1])):.2f}\")\n",
    "print(f\"Margin of decision boundary 2: {margin(X, b=-0.1, w=np.array([-0.6, -1])):.2f}\")\n",
    "print(f\"Margin of decision boundary 3: {margin(X, b=-0.5, w=np.array([0.2, -1])):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our intuition that the DB 2 was the best of the three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loc(X, b, w):\n",
    "    return np.argmin(np.abs(X @ w.reshape(-1, 1) + b) / np.linalg.norm(w))\n",
    "\n",
    "\n",
    "s1 = margin_loc(X, b=0.2, w=np.array([0.5, -1]))\n",
    "s2 = margin_loc(X, b=-0.1, w=np.array([-0.6, -1]))\n",
    "s3 = margin_loc(X, b=-0.5, w=np.array([0.2, -1]))\n",
    "\n",
    "plt.scatter(\n",
    "    X[[s1, s2, s3], 0],\n",
    "    X[[s1, s2, s3], 1],\n",
    "    c=np.where(y[[s1, s2, s3]] == -1, \"tab:orange\", \"tab:blue\"),\n",
    ")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"), alpha=0.1)\n",
    "\n",
    "x1 = np.linspace(-4, 4, 100)\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=0.2, w=np.array([0.5, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dotted\",\n",
    "    label=\"DB 1\",\n",
    ")\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=-0.1, w=np.array([-0.6, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"DB 2\",\n",
    ")\n",
    "plt.plot(\n",
    "    x1,\n",
    "    decision_boundary(x1, b=-0.5, w=np.array([0.2, -1])),\n",
    "    color=\"k\",\n",
    "    linestyle=\"dashdot\",\n",
    "    label=\"DB 3\",\n",
    ")\n",
    "\n",
    "\n",
    "def quiver_to_boundary(X, y, b, w, s):\n",
    "    Q = np.array([0, decision_boundary(0, b=b, w=w)])\n",
    "    proj = (np.dot(X[s] - Q, w) / np.linalg.norm(w)) * (w / np.linalg.norm(w))\n",
    "    plt.quiver(\n",
    "        [X[s][0]],\n",
    "        [X[s][1]],\n",
    "        [-proj[0]],\n",
    "        [-proj[1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=np.where(y[[s]] == -1, \"tab:orange\", \"tab:blue\"),\n",
    "    )\n",
    "\n",
    "\n",
    "quiver_to_boundary(X, y, b=0.2, w=np.array([0.5, -1]), s=s1)\n",
    "quiver_to_boundary(X, y, b=-0.1, w=np.array([-0.6, -1]), s=s2)\n",
    "quiver_to_boundary(X, y, b=-0.5, w=np.array([0.2, -1]), s=s3)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Points responsible for the margin of each decision boundary\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of SVM is to find a decision boundary that maximizes the margin between two classes. \n",
    "\n",
    "But that alone is not sufficient. We also need to ensure that the points lie on the correct side of the boundary.\n",
    "\n",
    "It becomes a constrained optimization problem.\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\max \\limits_{w, b} \\gamma(w, b) \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 0 \\text{ } \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "Let's substitute the definition of margin and we get\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\max \\limits_{w, b} \\min\\cfrac{|wx_i+b|}{\\|w\\|} \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 0 \\text{ } \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "Without a constraint on $w$ and $b$, we would have infinite possible values for these parameters that would give the same separating decision boundary due to its scale invariance.\n",
    "\n",
    "We can set a scale for $w$ and $b$ by adding the additional constraint $\\min|wx_i+b| = 1$.\n",
    "\n",
    "In other words, the scale of the values of $w$ and $b$ will be such that the smallest distance between the points and the decision boundary is $\\cfrac{1}{\\|w\\|}$.\n",
    "\n",
    "Why is the constant set to 1 and not another number?\n",
    "\n",
    "Well, because this allows us to remove it from $\\max \\limits_{w, b} \\min\\cfrac{|wx_i+b|}{\\|w\\|} \\Longrightarrow \\max \\limits_{w, b} \\cfrac{1}{\\|w\\|} \\text{ s.t. } \\min|wx_i+b| = 1$.\n",
    "\n",
    "So after putting it all together we have\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\max \\limits_{w, b} \\cfrac{1}{\\|w\\|} \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 0 \\\\ \n",
    "\\quad \\quad \\quad \\quad \\quad \\min|wx_i+b| = 1 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "We can combine the two constraints into one and obtain\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\max \\limits_{w, b} \\cfrac{1}{\\|w\\|} \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "Furthermore, $\\max\\cfrac{1}{\\|w\\|}$ is equivalent to $\\min\\|w\\|$, which in turn is equivalent to $\\min \\|w\\|^2$ which we can rewrite as dot product.\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\min \\limits_{w, b} w \\cdot w \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "To simplify the optimization even further we can introduce a factor of $\\cfrac{1}{2}$ so that the derivative becomes $w$ instead of $2w$.\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\min \\limits_{w, b} \\cfrac{1}{2} w \\cdot w \\\\\n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "During the optimization for SVM, we must estimate all 3 parameters: $w_1$, $w_2$ and $b$. \n",
    "\n",
    "This may seem counter intuitive given that a line inherently has only 2 degrees of freedom (slope and intercept).\n",
    "\n",
    "However, the constraint $\\min|wx_i+b| = 1$ ensures that only one scale-invariant decision boundary can be selected, making $w_1$, $w_2$ and $b$ uniquely identifiable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_loss(params):\n",
    "    w = params[: X.shape[1]]\n",
    "    return 0.5 * np.dot(w, w)\n",
    "\n",
    "\n",
    "def hard_constraint(params, X, y, i):\n",
    "    w = params[: X.shape[1]]\n",
    "    b = params[X.shape[1]]\n",
    "    return y[i] * (np.dot(w, X[i]) + b) - 1\n",
    "\n",
    "\n",
    "def parallel_thru_point(x1, w, p):\n",
    "    return p[1] - w[0] * (x1 - p[0]) / w[1]\n",
    "\n",
    "\n",
    "hard_initial = np.random.uniform(0, 1, 3)\n",
    "hard_cons = [\n",
    "    {\"type\": \"ineq\", \"fun\": hard_constraint, \"args\": (X, y, i)}\n",
    "    for i in range(X.shape[0])\n",
    "]\n",
    "\n",
    "hard_result = minimize(fun=hard_loss, x0=hard_initial, constraints=hard_cons)\n",
    "\n",
    "if np.allclose(hard_initial, hard_result.x):\n",
    "    raise ValueError(\"Initial values close to final estimates\")\n",
    "\n",
    "w = hard_result.x[: X.shape[1]]\n",
    "b = hard_result.x[X.shape[1]]\n",
    "sv = X[np.isclose(np.abs(X @ w.reshape(-1, 1) + b), 1.0, atol=1e-6).squeeze()]\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"))\n",
    "# plot decision boundary\n",
    "x1 = np.linspace(-4, 4, 100)\n",
    "plt.plot(x1, (-w[0] * x1 - b) / w[1], color=\"k\", alpha=0.5)\n",
    "# plot parallels thru support vectors\n",
    "plt.plot(\n",
    "    x1, parallel_thru_point(x1, w, sv[0]), color=\"k\", linestyle=\"dashed\", alpha=0.25\n",
    ")\n",
    "plt.plot(\n",
    "    x1, parallel_thru_point(x1, w, sv[1]), color=\"k\", linestyle=\"dashed\", alpha=0.25\n",
    ")\n",
    "# highlight support vectors\n",
    "plt.scatter(\n",
    "    sv[:, 0],\n",
    "    sv[:, 1],\n",
    "    s=80,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"y\",\n",
    "    color=\"y\",\n",
    ")\n",
    "# plot margin\n",
    "Q = np.array([0, decision_boundary(0, b=b, w=w)])\n",
    "proj_sv = (np.dot(sv[0] - Q, w) / np.linalg.norm(w)) * (w / np.linalg.norm(w))\n",
    "plt.quiver(\n",
    "    [Q[0], Q[0]],\n",
    "    [Q[1], Q[1]],\n",
    "    [proj_sv[0], -proj_sv[0]],\n",
    "    [proj_sv[1], -proj_sv[1]],\n",
    "    angles=\"xy\",\n",
    "    scale=1,\n",
    "    scale_units=\"xy\",\n",
    "    color=\"k\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.annotate(r\"$\\dfrac{1}{\\|w\\|}$\", [Q[0] - 1.1, Q[1]], color=\"grey\")\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"SVM Decision Boundary\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {np.mean(np.sign(X @ w + b) == y):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that $\\min|wx_i+b| = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(margin(X, b=b, w=w) * np.linalg.norm(w), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a very simple example where the classes are perfectly separated, so that we could use the hard margin implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "group_1_centroid = [-1, -1]\n",
    "group_2_centroid = [1, 1]\n",
    "X = np.r_[\n",
    "    rng.standard_normal((20, 2)) + group_1_centroid,\n",
    "    rng.standard_normal((20, 2)) + group_2_centroid,\n",
    "]\n",
    "y = np.array([-1] * 20 + [1] * 20)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Data with 2 classes (imperfect linear separation)\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the hard margin implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_initial = np.random.uniform(0, 1, 3)\n",
    "hard_cons = [\n",
    "    {\"type\": \"ineq\", \"fun\": hard_constraint, \"args\": (X, y, i)}\n",
    "    for i in range(X.shape[0])\n",
    "]\n",
    "\n",
    "hard_result = minimize(fun=hard_loss, x0=hard_initial, constraints=hard_cons)\n",
    "\n",
    "if np.allclose(hard_initial, hard_result.x):\n",
    "    raise ValueError(\"Initial values close to final estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data the hard margin optimizer doesn't work.\n",
    "\n",
    "We need to give it some slack.\n",
    "\n",
    "Let's introduce the slack variables $\\zeta_i$\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\min \\limits_{w, b, \\zeta} \\cfrac{1}{2} w \\cdot w \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\zeta_i \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "This way we allow for violations of the constraint. More specifically $\\zeta_i$ can take the following values:\n",
    "\n",
    "$\\zeta_i=0$: The point $x_i$ is correctly classified and outside the margin.\n",
    "\n",
    "$0<\\zeta_i \\le 1$: The point $x_i$ is correctly classified *but* within the margin.\n",
    "\n",
    "$\\zeta_i>1$: The point $x_i$ is misclassified.\n",
    "\n",
    "Thus, $\\zeta_i$ is constrained to be non-negative:\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\min \\limits_{w, b, \\zeta} \\cfrac{1}{2} w \\cdot w \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\zeta_i \\\\\n",
    "\\quad \\quad \\quad \\quad \\quad \\zeta_i \\ge 0 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "The problem is that a large enough $\\zeta_i$ defeats the purpose of the constraint.\n",
    "\n",
    "We can then penalize solutions that have large values of $\\zeta_i$. Namely, we can add an L1 regularization for $\\zeta_i$ and a regularization parameter $C$.\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\min \\limits_{w, b, \\zeta} \\cfrac{1}{2} w \\cdot w + C\\sum^m_i\\zeta_i \\\\ \n",
    "\\text{subject to} \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\zeta_i \\\\ \n",
    "\\quad \\quad \\quad \\quad \\quad \\zeta_i \\ge 0 \\quad \\forall i = 1, \\dots, m\n",
    "\\end{array}$\n",
    "\n",
    "$C$ provides a trade-off between maximizing the margin (prioritizing generalization) and minimizing misclassification.\n",
    "\n",
    "In the extreme case where $C = 0$, the objective function to be maximized is the same as in the hard margin SVM.\n",
    "\n",
    "However, there is effectively no constraint on classification accuracy, as the slack variables are free to get as large as needed.\n",
    "\n",
    "With a large $C$, solutions with large $\\zeta_i$ values are heavily penalized. This leads to a narrower margin and to over-fitting because fewer misclassification are allowed.\n",
    "\n",
    "A small $C$ allows for a wider margin, but at the cost of permitting more misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_margin_svm(X, y, C):\n",
    "    def soft_loss(params):\n",
    "        w = params[: X.shape[1]]\n",
    "        z = params[X.shape[1] + 1 :]\n",
    "        return 0.5 * np.dot(w, w) + C * np.sum(z)\n",
    "\n",
    "    def soft_constraint(params, X, y, i):\n",
    "        w = params[: X.shape[1]]\n",
    "        b = params[X.shape[1]]\n",
    "        zi = params[X.shape[1] + 1 + i]\n",
    "        return y[i] * (np.dot(w, X[i]) + b) - 1 + zi\n",
    "\n",
    "    soft_initial = np.random.uniform(0, 1, X.shape[1] + 1 + X.shape[0])\n",
    "    soft_cons = [\n",
    "        {\"type\": \"ineq\", \"fun\": soft_constraint, \"args\": (X, y, i)}\n",
    "        for i in range(X.shape[0])\n",
    "    ]\n",
    "    bounds_w = [(None, None)] * X.shape[1]\n",
    "    bounds_b = [(None, None)]\n",
    "    bounds_z = [(0, None)] * X.shape[0]\n",
    "    bounds = bounds_w + bounds_b + bounds_z\n",
    "\n",
    "    soft_result = minimize(\n",
    "        fun=soft_loss, x0=soft_initial, constraints=soft_cons, bounds=bounds\n",
    "    )\n",
    "\n",
    "    if np.allclose(soft_initial, soft_result.x):\n",
    "        raise ValueError(\"Initial values close to final estimates\")\n",
    "\n",
    "    w = soft_result.x[: X.shape[1]]\n",
    "    b = soft_result.x[X.shape[1]]\n",
    "    z = soft_result.x[X.shape[1] + 1 :]\n",
    "    # indices of within-margin zetas sorted by farthest to closest\n",
    "    svz = np.argwhere((z > 1e-6) & (z <= 1)).squeeze()[\n",
    "        np.argsort(z[(z > 1e-6) & (z <= 1)])\n",
    "    ]\n",
    "    zX = X[svz]\n",
    "    # within margin support vectors farthest from decision boundary (one per class)\n",
    "    sv = np.vstack((zX[y[svz] > 0][0], zX[y[svz] < 0][0]))\n",
    "\n",
    "    # plot data\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.where(y == -1, \"tab:orange\", \"tab:blue\"))\n",
    "    # plot decision boundary\n",
    "    x1 = np.linspace(-4, 4, 100)\n",
    "    plt.plot(x1, (-w[0] * x1 - b) / w[1], color=\"k\", alpha=0.5)\n",
    "    # plot parallels thru support vectors\n",
    "    plt.plot(\n",
    "        x1, parallel_thru_point(x1, w, sv[0]), color=\"k\", linestyle=\"dashed\", alpha=0.25\n",
    "    )\n",
    "    plt.plot(\n",
    "        x1, parallel_thru_point(x1, w, sv[1]), color=\"k\", linestyle=\"dashed\", alpha=0.25\n",
    "    )\n",
    "    # highlight support vectors\n",
    "    plt.scatter(\n",
    "        zX[:, 0],\n",
    "        zX[:, 1],\n",
    "        s=80,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"y\",\n",
    "        color=\"y\",\n",
    "    )\n",
    "    # plot margin\n",
    "    Q = np.array([0, decision_boundary(0, b=b, w=w)])\n",
    "    proj_svp = (np.dot(sv[0] - Q, w) / np.linalg.norm(w)) * (w / np.linalg.norm(w))\n",
    "    proj_svn = (np.dot(sv[1] - Q, w) / np.linalg.norm(w)) * (w / np.linalg.norm(w))\n",
    "    plt.quiver(\n",
    "        [Q[0], Q[0]],\n",
    "        [Q[1], Q[1]],\n",
    "        [proj_svp[0], proj_svn[0]],\n",
    "        [proj_svp[1], proj_svn[1]],\n",
    "        angles=\"xy\",\n",
    "        scale=1,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"k\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(f\"Soft SVM Decision Boundary C={C:.2f}\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Accuracy: {np.mean(np.sign(X @ w + b) == y):.0%}\")\n",
    "\n",
    "\n",
    "soft_margin_svm(X, y, C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a smaller value for $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_margin_svm(X, y, C=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
